import tkinter as tk

from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes
from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes
from msrest.authentication import CognitiveServicesCredentials

from array import array
import os
from PIL import Image
import requests
from io import BytesIO
from urllib.parse import urlparse
import sys
import time
import azure.cognitiveservices.speech as speechsdk
import pygame
import openai
import threading


'''
Authenticate
Authenticates your credentials and creates a client.
'''
#OpenAI API Key
openai.api_key = "YOUR OPENAI API KEY (eg. sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx)"  

#Azure Vision Key
subscription_key = "YOUR AZURE VISION RESOURCE KEY"
endpoint = "YOUR AZURE VISION RESOURCE END POINT (eg. https://yourname.cognitiveservices.azure.com/)"
computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))

#Azure Speech Key
speech_key = 'YOUR SPEECH RESOURCE KEY'
service_region = 'YOUR REGION (eg. westeurope)'

#You can choose the voice that is used here
speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
speech_config.speech_synthesis_voice_name = "en-US-DavisNeural"     

'''
END - Authenticate
'''


#OCR Stuff

def OCR (url):
    '''
    OCR: Read File using the Read API, extract text - remote
    This example will extract text in an image, then print results, line by line.
    This API call can also extract handwriting style text (not shown).
    '''
    print("===== Read File - remote =====")
    # Get an image with text
    read_image_url = url

    # Call API with URL and raw response (allows you to get the operation location)
    read_response = computervision_client.read(read_image_url,  raw=True)

    # Get the operation location (URL with an ID at the end) from the response
    read_operation_location = read_response.headers["Operation-Location"]
    # Grab the ID from the URL
    operation_id = read_operation_location.split("/")[-1]

    # Call the "GET" API and wait for it to retrieve the results 
    while True:
        read_result = computervision_client.get_read_result(operation_id)
        if read_result.status not in ['notStarted', 'running']:
            break
        time.sleep(1)

    # Print the detected text, line by line
    result = ""

    if read_result.status == OperationStatusCodes.succeeded:
        for text_result in read_result.analyze_result.read_results:
            for line in text_result.lines:
            #  print(line.text)
            #  print(line.bounding_box)
                result += line.text + " "

    return result

def ImageRecognize (url, additionalWords, textInImage):

    '''
    Quickstart variables
    These variables are shared by several examples
    '''
    # Images used for the examples: Describe an image, Categorize an image, Tag an image, 
    # Detect faces, Detect adult or racy content, Detect the color scheme, 
    # Detect domain-specific content, Detect image types, Detect objects
    # images_folder = os.path.join (os.path.dirname(os.path.abspath(__file__)), "images")
    remote_image_url = url
    '''
    END - Quickstart variables
    '''


    '''
    Tag an Image - remote
    This example returns a tag (key word) for each thing in the image.
    '''
    print("===== Tag an image - remote =====")
    # Call API with remote image
    tags_result_remote = computervision_client.tag_image(remote_image_url )

    # Print results with confidence score
    print("Tags in the remote image: ")
    if (len(tags_result_remote.tags) == 0):
        print("No tags detected.")
    else:
        output = "'"
        for tag in tags_result_remote.tags:
            print("'{}' with confidence {:.2f}%".format(tag.name, tag.confidence * 100))
            output += tag.name + ", "
        output += "'"

            
    addition = additionalWords



    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": "Describe a scene based on these words '{} {} {}'. Don't mention people if they are not in the word set. Ignore the word 'Human' ".format(output, addition, textInImage)}
        ]
    )


    return completion.choices[0].message.content, output

    '''
    END - Tag an Image - remote
     '''
    



def ValidateUrl(inputText):
    try:
        result = urlparse(inputText)
        if all([result.scheme, result.netloc]):
            
            response = requests.get(inputText)
            image_data = BytesIO(response.content)
            

            try:
                image = Image.open(image_data)
            except IOError:
                print("The URL does not point to a valid image file.")
                return False
            else:
                print("The URL points to a valid image file.")
                return True

        else:
            print("Invalid URL")
            return False
    except ValueError:
        print("Invalid URL")
        return False
    


    
def play_audio(audio_filename):
    try:
        pygame.mixer.init()
        pygame.mixer.music.load(audio_filename)

    except Exception as e:
        print(f"Error playing audio: {e}")

    finally:
        # Clean up the mixer
        pygame.mixer.quit()


def Speak(text):
    print(f"Speaking {text}")

    if len(text) < 10:
        return

    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)
    text_to_synthesize = text
    result = speech_synthesizer.speak_text_async(text_to_synthesize).get()

    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        audio_data = result.audio_data
        with open('output_audio.wav', 'wb') as audio_file:
            audio_file.write(audio_data)

        audio_filename = 'output_audio.wav'

        # Start playing audio in a separate thread
        audio_thread = threading.Thread(target=play_audio, args=(audio_filename,))
        audio_thread.start()

        # Wait for the audio thread to finish
        audio_thread.join()

    else:
        print(f"Error synthesizing audio: {result.reason}")




class Application(tk.Frame):

    textToSpeak = "Testing audio"

    def __init__(self, master=None):
        super().__init__(master)
        self.master = master
        self.master.title("Input Application")
        self.master.geometry("600x600")
        self.create_widgets()

    def create_widgets(self):
        self.input_label = tk.Label(self.master, text="Enter Input:")
        self.input_label.pack(side="top")


        self.input_box = tk.Entry(self.master, width=40)
        self.input_box.pack(pady=10, padx=10, fill="x")
        self.input_box.pack_propagate(0)

        self.addWordsLabel = tk.Label(self.master, text="Additional Words:")
        self.addWordsLabel.pack(side="top")

        self.words_box = tk.Entry(self.master, width=40)
        self.words_box.pack(pady=10, fill="x")
        self.words_box.pack_propagate(0)


        # Button frame
        button_frame = tk.Frame(self.master)
        button_frame.pack(side=tk.TOP, pady=5, fill="x")

        # Add Input button
        self.add_input_button = tk.Button(button_frame, text="Analyse", command=self.add_input)
        self.add_input_button.pack( padx=5)

        # Speak button
        self.stop_button = tk.Button(button_frame, text="Speak", command=self.doSpeak)
        self.stop_button.pack(padx=5)

        self.output_label = tk.Label(self.master, text="Output:")
        self.output_label.pack(side="top")

        self.output_box = tk.Text(self.master, height=10, width=40, wrap="word")
        self.output_box.pack(pady=10, fill="both", expand=True)
        self.output_box.pack_propagate(0)


        self.quit_button = tk.Button(self.master, text="Quit", command=self.stop_running)
        self.quit_button.pack(pady=10, padx=10)
        # Bind the Enter key to the add_input method
        self.master.bind('<Return>', self.add_input)


    def add_input(self, event=None):
        input_text = self.input_box.get()
        additionalWords = self.words_box.get()
        self.output_box.delete("1.0", tk.END)
        self.output_box.insert(tk.END, "Analyzing " + "\n\n")
        self.master.update()
        
        if(ValidateUrl(input_text)):
            
            textInImage = OCR (input_text)
            result, objectsInImage = ImageRecognize (input_text, additionalWords,textInImage)
            self.textToSpeak = result
            self.output_box.insert(tk.END,"Words found: \n"  + textInImage + "\n")
            self.output_box.insert(tk.END,"Objects found: \n"  + objectsInImage + "\n\n")
            self.master.update()
            self.output_box.insert(tk.END,"Result: \n\n"  + result + "\n\n")

            self.output_box.insert(tk.END,  + "\n")

        else:
            self.output_box.insert(tk.END, "Bad URL...")            
            self.input_box.delete(0, tk.END)
            self.master.update()

    def doSpeak (self):
        Speak(self.textToSpeak)
            

    def stop_running(self):
        self.master.destroy()
        self.master.quit()

    






root = tk.Tk()
app = Application(master=root)
app.mainloop()
